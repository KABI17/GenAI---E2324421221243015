{
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'language-identification-datasst:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F94159%2F218863%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240326%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240326T075212Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7a103813235a42df3f931106d7e51b8ef6fb53fedab90a5588eafc1b44752f7d568aa5f487428a73edb517b67ffb1ae53be6e71279e07dd62c7ebb4f6d62ecaff6039366b8e2a72e26b4aaff8ca88fe182f87063d554ddaaeb6464a6ecae6821c552650177a299f77de2b82b57bd8fbd2e88ca6f94c7f3b3a0a090373e013546eeaf710672e3701c77fa28f20a6a430367c2234ab9bc6e9d80e62c2b63014ce769ffe3f99f7b531c3a2054691ae54475a35146ddea3c1676def0e4825bbeaaa0d562d6eeeb1b3503f66f01251bef28f8b90818e84bfdb965eec35b3fa05ac2f2b30b693626317ee73dd8f93a5bd93a4337d19ad1cfdcc33b3aba9134b1119e98'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "Dg5GE0bNvt-M",
        "outputId": "4bc63cce-5929-4d74-dc46-2f824eb50b49",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading language-identification-datasst, 5798108 bytes compressed\n",
            "[==================================================] 5798108 bytes downloaded\n",
            "Downloaded and uncompressed: language-identification-datasst\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "metadata": {
        "id": "ZOm3Jno8vt-Q"
      },
      "cell_type": "markdown",
      "source": [
        "# Preface\n",
        "First of all, **thanks a lot for this fantastic dataset!**\n",
        "\n",
        "It is very interesting, as it contains a lot of texts. Most of the rows consist of *two languages* - a **primary** one for the major part and a **secondary** one used for some words only.\n",
        "\n",
        "According to this observation, we already have a lot of noise in our data (i.e. all words in the secondary language).\n",
        "\n",
        "A part from that, we are *not* dealing with long texts here, all texts are relatively short.\n",
        "\n",
        "So, we are not working on a plain vanilla problem using the 10 best text books available for each of the investigated languages, but on a *realistic scenario* using kind of real world data.\n",
        "\n",
        "# Pupose\n",
        "This notebook, should show alternative ways, how to detect the language.\n",
        "\n",
        "The focus lies on the identification of efficient algorithms (in terms of memory requirements) by exploiting the knowledge about languages and their structures.\n",
        "\n",
        "# Agenda\n",
        "1. Get familiar with the data\n",
        "2. Language specifics *aka Uni- vs. Bi-Grams*\n",
        "3. Naive Bayes\n",
        "4. k Nearest Neighbours\n",
        "5. Sum of squared differences\n",
        "6. Kulback Leibler Divergence\n",
        "7. Kolmogorov Smirnov Test\n",
        "8. Analysis & Comparison of F1-Scores\n",
        "\n",
        "# Get familiar with the data"
      ]
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true,
        "id": "zKR4JeQSvt-S",
        "outputId": "abba25c7-0d10-4552-91d9-2c1d0fe70b4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "raw = pd.read_csv('../input/language-identification-datasst/dataset.csv')\n",
        "raw"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                    Text  language\n",
              "0      klement gottwaldi surnukeha palsameeriti ning ...  Estonian\n",
              "1      sebes joseph pereira thomas  på eng the jesuit...   Swedish\n",
              "2      ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...      Thai\n",
              "3      விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...     Tamil\n",
              "4      de spons behoort tot het geslacht haliclona en...     Dutch\n",
              "...                                                  ...       ...\n",
              "21995  hors du terrain les années  et  sont des année...    French\n",
              "21996  ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...      Thai\n",
              "21997  con motivo de la celebración del septuagésimoq...   Spanish\n",
              "21998  年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...   Chinese\n",
              "21999   aprilie sonda spațială messenger a nasa și-a ...  Romanian\n",
              "\n",
              "[22000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0c689cf1-bbc4-46bf-bf0f-2be77aca9ae4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>language</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>klement gottwaldi surnukeha palsameeriti ning ...</td>\n",
              "      <td>Estonian</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sebes joseph pereira thomas  på eng the jesuit...</td>\n",
              "      <td>Swedish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>விசாகப்பட்டினம் தமிழ்ச்சங்கத்தை இந்துப் பத்திர...</td>\n",
              "      <td>Tamil</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>de spons behoort tot het geslacht haliclona en...</td>\n",
              "      <td>Dutch</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21995</th>\n",
              "      <td>hors du terrain les années  et  sont des année...</td>\n",
              "      <td>French</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21996</th>\n",
              "      <td>ใน พศ  หลักจากที่เสด็จประพาสแหลมมลายู ชวา อินเ...</td>\n",
              "      <td>Thai</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21997</th>\n",
              "      <td>con motivo de la celebración del septuagésimoq...</td>\n",
              "      <td>Spanish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21998</th>\n",
              "      <td>年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由...</td>\n",
              "      <td>Chinese</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21999</th>\n",
              "      <td>aprilie sonda spațială messenger a nasa și-a ...</td>\n",
              "      <td>Romanian</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22000 rows × 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0c689cf1-bbc4-46bf-bf0f-2be77aca9ae4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0c689cf1-bbc4-46bf-bf0f-2be77aca9ae4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0c689cf1-bbc4-46bf-bf0f-2be77aca9ae4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-313f1142-ffbd-4a9e-b63e-ea1e4b0ac343\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-313f1142-ffbd-4a9e-b63e-ea1e4b0ac343')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-313f1142-ffbd-4a9e-b63e-ea1e4b0ac343 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_ca51a9ee-9efa-4b64-8db0-1ea479295f97\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('raw')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ca51a9ee-9efa-4b64-8db0-1ea479295f97 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('raw');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "raw",
              "summary": "{\n  \"name\": \"raw\",\n  \"rows\": 22000,\n  \"fields\": [\n    {\n      \"column\": \"Text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 21859,\n        \"samples\": [\n          \"semua kucing dalam genus ini berbagi nenek moyang yang sama yang mungkin hidup sekitar \\u2013 juta tahun yang lalu di asia hubungan yang tepat dalam felidae dekat tetapi masih belum pasti misalnya kucing gunung cina kadang-kadang diklasifikasikan dengan nama felis silvestris bieti sebagai upaspesies kucing liar seperti varietas afrika utara f s lybica\",\n          \"not completely happy with their temporary name in the summer of  a sign caught citos eye just outside the freestate town of kroonstad pointing to a little town called \\\"wonderboom\\\" in  danny de wet left the band and was replaced by garth mcleod formerly of respected south african rock band sugardrive in  garth mcleod was killed in a motorcycle accident and was replaced by jonathan bell\",\n          \"tetraglenes bucculenta \\u00e4r en skalbaggsart som beskrevs av charles joseph gahan  tetraglenes bucculenta ing\\u00e5r i sl\\u00e4ktet tetraglenes och familjen l\\u00e5nghorningar\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"language\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"Estonian\",\n          \"Korean\",\n          \"Urdu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aFt_SfQhvt-T",
        "outputId": "517279f1-14f0-41a6-8909-a2b5fbe02601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "# Languages\n",
        "languages = set(raw['language'])\n",
        "print('Languages', languages)\n",
        "print('========')\n",
        "# Examples of multiple langs taken from heads and tails\n",
        "print('Swedish & English:', raw['Text'][1])\n",
        "print('Thai & English:', raw['Text'][2])\n",
        "print('Chinese & English:', raw['Text'][21998])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Languages {'French', 'Dutch', 'Chinese', 'Latin', 'Urdu', 'Japanese', 'Thai', 'Russian', 'Portugese', 'Spanish', 'Persian', 'Turkish', 'Indonesian', 'English', 'Romanian', 'Arabic', 'Swedish', 'Estonian', 'Korean', 'Hindi', 'Pushto', 'Tamil'}\n",
            "========\n",
            "Swedish & English: sebes joseph pereira thomas  på eng the jesuits and the sino-russian treaty of nerchinsk  the diary of thomas pereira bibliotheca instituti historici s i --   rome libris \n",
            "Thai & English: ถนนเจริญกรุง อักษรโรมัน thanon charoen krung เริ่มตั้งแต่ถนนสนามไชยถึงแม่น้ำเจ้าพระยาที่ถนนตก กรุงเทพมหานคร เป็นถนนรุ่นแรกที่ใช้เทคนิคการสร้างแบบตะวันตก ปัจจุบันผ่านพื้นที่เขตพระนคร เขตป้อมปราบศัตรูพ่าย เขตสัมพันธวงศ์ เขตบางรัก เขตสาทร และเขตบางคอแหลม\n",
            "Chinese & English: 年月，當時還只有歲的她在美國出道，以mai-k名義推出首張英文《baby i like》，由美國的獨立廠牌bip·record發行，以外國輸入盤的形式在日本發售，旋即被抢购一空。其後於月日發行以倉木麻衣名義發行的首張日文單曲《love day after tomorrow》，正式於日本出道。這張單曲初動銷量只得約萬張，可是其後每週銷量一直上升，並於年月正式突破百萬銷量，合计万张。成為年最耀眼的新人歌手。\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "V6v6LjgTvt-T"
      },
      "cell_type": "markdown",
      "source": [
        "Well, if you inspect the data yourself, you will find plenty more examples containing two languages.\n",
        "\n",
        "Now, let's do, what we always need to do when applying ML somewhere: *Split the data into train and test.*"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "1mPEy38Mvt-U"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X=raw['Text']\n",
        "y=raw['language']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(len(X_train))\n",
        "print(len(X_test))\n",
        "print(len(y_train))\n",
        "print(len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UTwGxKuwvt-U"
      },
      "cell_type": "markdown",
      "source": [
        "# Language specifics\n",
        "\n",
        "To be memory efficient, we need to restrict ourselves to a minimum number of features, which is still sufficient to distinguish between the languages.\n",
        "\n",
        "## Excurs on Cryptographie\n",
        "When thinking about a solution, the first thing, which came into my mind, was the lecture on Crypto-Analysis. Breaking a [Vigenère-](https://en.wikipedia.org/wiki/Vigen%C3%A8re_cipher) or [Ceasar-Cipher](https://en.wikipedia.org/wiki/Caesar_cipher) is performed by exploiting the statistical distribution of characters using [Frequency analysis](https://en.wikipedia.org/wiki/Frequency_analysis) on Uni-Grams, Bi-Grams or even Tri-Grams.\n",
        "\n",
        "Therefore I feel confident, that using one of these n-Grams ($n \\in {1,2,3}$) will be sufficient here.\n",
        "\n",
        "## Uni-Gram\n",
        "\n",
        "The smallest pieces of information are single characters, i.e. **Uni-Grams**.\n",
        "\n",
        "Using Uni-Grams allows us to identify *all* languages, which consist of *unique symbols*.\n",
        "E.g. this approach is sufficient to identify Chinese, Korean, Japanese, ...\n",
        "\n",
        "Let's inspect the unigrams:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "8CqU721Pvt-V"
      },
      "cell_type": "code",
      "source": [
        "# Extract Unigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "unigramVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,1))\n",
        "X_unigram_train_raw = unigramVectorizer.fit_transform(X_train)\n",
        "X_unigram_test_raw = unigramVectorizer.transform(X_test)\n",
        "\n",
        "\n",
        "unigramFeatures = unigramVectorizer.get_feature_names()\n",
        "\n",
        "print('Number of unigrams in training set:', len(unigramFeatures))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1SPTj6bPvt-W"
      },
      "cell_type": "markdown",
      "source": [
        "So, we have **6816** Uni-Grams.\n",
        "\n",
        "But how are they distributed across the languages?"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_KnQegqevt-W"
      },
      "cell_type": "code",
      "source": [
        "# Aggregate Unigrams per language\n",
        "def train_lang_dict(X_raw_counts, y_train):\n",
        "    lang_dict = {}\n",
        "    for i in range(len(y_train)):\n",
        "        lang = y_train[i]\n",
        "        v = np.array(X_raw_counts[i])\n",
        "        if not lang in lang_dict:\n",
        "            lang_dict[lang] = v\n",
        "        else:\n",
        "            lang_dict[lang] += v\n",
        "\n",
        "    # to relative\n",
        "    for lang in lang_dict:\n",
        "        v = lang_dict[lang]\n",
        "        lang_dict[lang] = v / np.sum(v)\n",
        "\n",
        "    return lang_dict\n",
        "\n",
        "language_dict_unigram = train_lang_dict(X_unigram_train_raw.toarray(), y_train.values)\n",
        "\n",
        "# Collect relevant chars per language\n",
        "def getRelevantCharsPerLanguage(features, language_dict, significance=1e-5):\n",
        "    relevantCharsPerLanguage = {}\n",
        "    for lang in languages:\n",
        "        chars = []\n",
        "        relevantCharsPerLanguage[lang] = chars\n",
        "        v = language_dict[lang]\n",
        "        for i in range(len(v)):\n",
        "            if v[i] > significance:\n",
        "                chars.append(features[i])\n",
        "    return relevantCharsPerLanguage\n",
        "\n",
        "relevantCharsPerLanguage = getRelevantCharsPerLanguage(unigramFeatures, language_dict_unigram)\n",
        "\n",
        "# Print number of unigrams per language\n",
        "for lang in languages:\n",
        "    print(lang, len(relevantCharsPerLanguage[lang]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dE4pgXARvt-X"
      },
      "cell_type": "markdown",
      "source": [
        "As we can see in the above overview, following languages are using a lot of unique symbols:\n",
        "- Chinese:  3,249\n",
        "- Japanese: 2,054\n",
        "- Korean:   1,407\n",
        "\n",
        "I.e. we can easily identify these languages by using Uni-Grams.\n",
        "\n",
        "But:\n",
        "- All other languages are using **much fewer symbols**.\n",
        "- And most of the other languages **share common symbols**.\n",
        "\n",
        "Well, the Uni-Gram-approach might also be usefull, if languages share *common symbols*, like the [Latin symbols](https://en.wikipedia.org/wiki/Latin_alphabet) which are used in a lot of [Indo-European](https://en.wikipedia.org/wiki/Indo-European_languages) languages including e.g. [Romance](https://en.wikipedia.org/wiki/Romance_languages) (Italian, Romanian, Spanish, ...) and [Germanic](https://en.wikipedia.org/wiki/Germanic_languages) (English, German, Dutch, ...). And in a similar way also for [Kyrillic letters](https://en.wikipedia.org/wiki/Cyrillic_alphabets) used for Russian, Serbian, ...\n",
        "\n",
        "In this cases we need to use the distribution of the characters, which will allow us to distinguish between the root languages. So we will be able to distinguish between a language based on Romance (e.g. Italian) or a Germanic (e.g. Dutch).\n",
        "\n",
        "Unfortunately, using **Uni-Grams** will not be sufficient in all cases. We might face some issues, when we try to distinguish between languages having the same root, like for the Romance languages: Italian, Spanish, French, Protuguese, Catalan, ... Or for the Germanic languages: English, Dutch, Swedish, ...\n",
        "\n",
        "So, let's have a look on some European languages, sharing Latin symbols"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_FS7eCfTvt-X"
      },
      "cell_type": "code",
      "source": [
        "# get most common chars for a few European languages\n",
        "europeanLanguages = ['Portugese', 'Spanish', 'Latin', 'English', 'Dutch', 'Swedish']\n",
        "relevantChars_OnePercent = getRelevantCharsPerLanguage(unigramFeatures, language_dict_unigram, 1e-2)\n",
        "\n",
        "# collect and sort chars\n",
        "europeanCharacters = []\n",
        "for lang in europeanLanguages:\n",
        "    europeanCharacters += relevantChars_OnePercent[lang]\n",
        "europeanCharacters = list(set(europeanCharacters))\n",
        "europeanCharacters.sort()\n",
        "\n",
        "# build data\n",
        "indices = [unigramFeatures.index(f) for f in europeanCharacters]\n",
        "data = []\n",
        "for lang in europeanLanguages:\n",
        "    data.append(language_dict_unigram[lang][indices])\n",
        "\n",
        "#build dataframe\n",
        "df = pd.DataFrame(np.array(data).T, columns=europeanLanguages, index=europeanCharacters)\n",
        "df.index.name = 'Characters'\n",
        "df.columns.name = 'Languages'\n",
        "\n",
        "# plot heatmap\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "sn.set(font_scale=0.8) # for label size\n",
        "sn.set(rc={'figure.figsize':(10, 10)})\n",
        "sn.heatmap(df, cmap=\"Greens\", annot=True, annot_kws={\"size\": 12}, fmt='.0%')# font size\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xUXHHMu6vt-X"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at the data above shows, that it is very hard to distinguish between the European languages. Especially between the languages based on Romance.\n",
        "\n",
        "Anyhow, it is also very hard to distinguish between English and Italian (labeled *Latin* in the table above), since there are only a few narrow differences in the distribution.\n",
        "\n",
        "We will inspect the performance (in terms of the F1) score in the later chapters.\n",
        "\n",
        "********\n",
        "\n",
        "## Bi-Gram\n",
        "The next larger piece of information are **Bi-Grams**. Unfortunately, using Bi-Grams might already blow up the required resources. So we should restrict ourselves here, to use only those,\n",
        "occurring most frequently.\n",
        "\n",
        "If we restrict ourselves to the most frequently n-Grams only, we will face another challenge. For languages containing a large amount of symbols, all combinations of symbols do not occure very often (e.g. Chinese contains >3,000 symbols). This seems to be a common pattern for languages based on syllables.\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aeZJH0bWvt-Y"
      },
      "cell_type": "code",
      "source": [
        "# number of bigrams\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bigramVectorizer = CountVectorizer(analyzer='char', ngram_range=(2,2))\n",
        "X_bigram_raw = bigramVectorizer.fit_transform(X_train)\n",
        "bigramFeatures = bigramVectorizer.get_feature_names()\n",
        "print('Number of bigrams', len(bigramFeatures))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oudmtqaDvt-Y"
      },
      "cell_type": "markdown",
      "source": [
        "But for the languages consisting on a few letters only, using Bi-Grams is very helpfull. Since the most frequently used Bi-Grams differ a lot.\n",
        "\n",
        "Whereas for Japanese and Chinese we can *not* find any Bi-Grams occuring at least in 1% of the cases."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "naSS1bSTvt-Y"
      },
      "cell_type": "code",
      "source": [
        "# top bigrams (>1%) for Spanish, Italian (Latin), English, Dutch, Chinese, Japanese, Korean\n",
        "language_dict_bigram = train_lang_dict(X_bigram_raw.toarray(), y_train.values)\n",
        "relevantCharsPerLanguage = getRelevantCharsPerLanguage(bigramFeatures, language_dict_bigram, significance=1e-2)\n",
        "print('Spanish', relevantCharsPerLanguage['Spanish'])\n",
        "print('Italian (Latin)', relevantCharsPerLanguage['Latin'])\n",
        "print('English', relevantCharsPerLanguage['English'])\n",
        "print('Dutch', relevantCharsPerLanguage['Dutch'])\n",
        "print('Chinese', relevantCharsPerLanguage['Chinese'])\n",
        "print('Japanese', relevantCharsPerLanguage['Japanese'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FKs-46Yzvt-Z"
      },
      "cell_type": "markdown",
      "source": [
        "Alternatively, we could also use a **mixture** of Uni-Grams and Bi-Grams, restricted on the most frequently used ones.\n",
        "\n",
        "## Mixture of Uni-Gram & Bi-Grams\n",
        "When we restrict ourselves to a limited number of features, it is important, that we will capture details for each language. Since Chinese consists of >3,000 of different symbols, the probability of the most frequently used Chinese Uni-Grams might be below the top 1000 used Bi-Grams of the other languages.\n",
        "\n",
        "In the following, I will try some arbitrarily chosen Mixtures. They are straight-forward ideas, based on intuition only.\n",
        "\n",
        "For a more analytic approach, one should use a dimension reduction technique or something similar.\n",
        "\n",
        "### Mixture Uni- & Bi-Grams (using the top 1%)\n",
        "So, first try, take Uni- & Bi-Grams occurring at least in 1% of all cases."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "W0LrIVnLvt-Z"
      },
      "cell_type": "code",
      "source": [
        "# Uni- & Bi-Gram Mixture CountVectorizer for top 1% features\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "top1PrecentMixtureVectorizer = CountVectorizer(analyzer='char', ngram_range=(1,2), min_df=1e-2)\n",
        "X_top1Percent_train_raw = top1PrecentMixtureVectorizer.fit_transform(X_train)\n",
        "X_top1Percent_test_raw = top1PrecentMixtureVectorizer.transform(X_test)\n",
        "\n",
        "language_dict_top1Percent = train_lang_dict(X_top1Percent_train_raw.toarray(), y_train.values)\n",
        "\n",
        "top1PercentFeatures = top1PrecentMixtureVectorizer.get_feature_names()\n",
        "print('Length of features', len(top1PercentFeatures))\n",
        "print('')\n",
        "\n",
        "#Unique features per language\n",
        "relevantChars_Top1Percent = getRelevantCharsPerLanguage(top1PercentFeatures, language_dict_top1Percent, 1e-5)\n",
        "for lang in relevantChars_Top1Percent:\n",
        "    print(\"{}: {}\".format(lang, len(relevantChars_Top1Percent[lang])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aaswUYSbvt-Z"
      },
      "cell_type": "markdown",
      "source": [
        "Well, we are using a lot of features per language, which might be already a good solution.\n",
        "\n",
        "*But chosing 3,079 features is already a lot*. And therefore the calculation is still expensive.\n",
        "\n",
        "By the way: It is already possible to start the calculation based on 3,079 features. And the delivered results seem to be sufficient.\n",
        "\n",
        "So, how can we do better?\n",
        "\n",
        "### Take top 50 (Uni- & Bi-Grams) per language\n",
        "Well, we can restrict ourselves to the top 50 Uni- & Bi-Grams per language.\n",
        "\n",
        "This will lead to max `22 * 50 = 1100` features.\n",
        "\n",
        "(*Remark:* Due to simplification, I will reuse `top1PrecentMixtureVectorizer` here, since it provides at least >500 features per language)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "BAVB3uNcvt-a"
      },
      "cell_type": "code",
      "source": [
        "def getRelevantGramsPerLanguage(features, language_dict, top=50):\n",
        "    relevantGramsPerLanguage = {}\n",
        "    for lang in languages:\n",
        "        chars = []\n",
        "        relevantGramsPerLanguage[lang] = chars\n",
        "        v = language_dict[lang]\n",
        "        sortIndex = (-v).argsort()[:top]\n",
        "        for i in range(len(sortIndex)):\n",
        "            chars.append(features[sortIndex[i]])\n",
        "    return relevantGramsPerLanguage\n",
        "\n",
        "top50PerLanguage_dict = getRelevantGramsPerLanguage(top1PercentFeatures, language_dict_top1Percent)\n",
        "\n",
        "# top50\n",
        "allTop50 = []\n",
        "for lang in top50PerLanguage_dict:\n",
        "    allTop50 += set(top50PerLanguage_dict[lang])\n",
        "\n",
        "top50 = list(set(allTop50))\n",
        "\n",
        "print('All items:', len(allTop50))\n",
        "print('Unique items:', len(top50))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S_vURu2zvt-a"
      },
      "cell_type": "markdown",
      "source": [
        "**So, when dealing with the top-50-approach on these 22 languages, we will effectively use 564 features only.**\n",
        "\n",
        "Now, let's build the data set for the models, based on our 564 features."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "VEi1ZCaevt-a"
      },
      "cell_type": "code",
      "source": [
        "# getRelevantColumnIndices\n",
        "def getRelevantColumnIndices(allFeatures, selectedFeatures):\n",
        "    relevantColumns = []\n",
        "    for feature in selectedFeatures:\n",
        "        relevantColumns = np.append(relevantColumns, np.where(allFeatures==feature))\n",
        "    return relevantColumns.astype(int)\n",
        "\n",
        "relevantColumnIndices = getRelevantColumnIndices(np.array(top1PercentFeatures), top50)\n",
        "\n",
        "\n",
        "X_top50_train_raw = np.array(X_top1Percent_train_raw.toarray()[:,relevantColumnIndices])\n",
        "X_top50_test_raw = X_top1Percent_test_raw.toarray()[:,relevantColumnIndices]\n",
        "\n",
        "print('train shape', X_top50_train_raw.shape)\n",
        "print('test shape', X_top50_test_raw.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jWeber7qvt-a"
      },
      "cell_type": "markdown",
      "source": [
        "Before we start to use the features in our models, let's finally discuss the feature selection.\n",
        "\n",
        "## n-Gram Discussion\n",
        "\n",
        "Let's recap the above:\n",
        "- using Uni-Grams is sufficient for syllable languages (Chinese, Japanese, ...)\n",
        "- but Uni-Grams do not help when inspecting European languages - here we need at least some Bi-Grams\n",
        "- using all Bi-Grams will blow up the memory required\n",
        "- the same is true for **Tri-Grams** - so, we will do *no* further investigation on that here\n",
        "\n",
        "Conclusion: **From a theoretical perspective, it is most efficient to use a Mixture of the most common Uni-Grams and Bi-Grams**.\n",
        "\n",
        "***\n",
        "\n",
        "# Naive Bayes\n",
        "Now, we can apply the Naive Bayes on our different feature sets:\n",
        "- Unigram (`X_unigram_train_raw`)\n",
        "- Mixture Top 1% (`X_top1Percent_train_raw`)\n",
        "- Mixture Top 50 (`X_top50_train_raw`)\n",
        "\n",
        "First of all, we need to define some utility functions for our evaluation."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "0q2hKdhavt-b"
      },
      "cell_type": "code",
      "source": [
        "# Define some functions for our purpose\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "\n",
        "# Utils for conversion of different sources into numpy array\n",
        "def toNumpyArray(data):\n",
        "    data_type = type(data)\n",
        "    if data_type == np.ndarray:\n",
        "        return data\n",
        "    elif data_type == list:\n",
        "        return np.array(data_type)\n",
        "    elif data_type == scipy.sparse.csr.csr_matrix:\n",
        "        return data.toarray()\n",
        "    print(data_type)\n",
        "    return None\n",
        "\n",
        "\n",
        "def normalizeData(train, test):\n",
        "    train_result = normalize(train, norm='l2', axis=1, copy=True, return_norm=False)\n",
        "    test_result = normalize(test, norm='l2', axis=1, copy=True, return_norm=False)\n",
        "    return train_result, test_result\n",
        "\n",
        "def applyNaiveBayes(X_train, y_train, X_test):\n",
        "    trainArray = toNumpyArray(X_train)\n",
        "    testArray = toNumpyArray(X_test)\n",
        "\n",
        "    clf = MultinomialNB()\n",
        "    clf.fit(trainArray, y_train)\n",
        "    y_predict = clf.predict(testArray)\n",
        "    return y_predict\n",
        "\n",
        "def plot_F_Scores(y_test, y_predict):\n",
        "    f1_micro = f1_score(y_test, y_predict, average='micro')\n",
        "    f1_macro = f1_score(y_test, y_predict, average='macro')\n",
        "    f1_weighted = f1_score(y_test, y_predict, average='weighted')\n",
        "    print(\"F1: {} (micro), {} (macro), {} (weighted)\".format(f1_micro, f1_macro, f1_weighted))\n",
        "\n",
        "def plot_Confusion_Matrix(y_test, y_predict, color=\"Blues\"):\n",
        "    allLabels = list(set(list(y_test) + list(y_predict)))\n",
        "    allLabels.sort()\n",
        "    confusionMatrix = confusion_matrix(y_test, y_predict, labels=allLabels)\n",
        "    unqiueLabel = np.unique(allLabels)\n",
        "    df_cm = pd.DataFrame(confusionMatrix, columns=unqiueLabel, index=unqiueLabel)\n",
        "    df_cm.index.name = 'Actual'\n",
        "    df_cm.columns.name = 'Predicted'\n",
        "\n",
        "    sn.set(font_scale=0.8) # for label size\n",
        "    sn.set(rc={'figure.figsize':(15, 15)})\n",
        "    sn.heatmap(df_cm, cmap=color, annot=True, annot_kws={\"size\": 12}, fmt='g')# font size\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ylt7VvJ5vt-b"
      },
      "cell_type": "markdown",
      "source": [
        "## Uni-Grams\n",
        "\n",
        "Remember, we are working on **6,816** features in the case of Uni-Grams."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "f-DmBPIZvt-b"
      },
      "cell_type": "code",
      "source": [
        "# Unigrams\n",
        "X_unigram_train, X_unigram_test = normalizeData(X_unigram_train_raw, X_unigram_test_raw)\n",
        "y_predict_nb_unigram = applyNaiveBayes(X_unigram_train, y_train, X_unigram_test)\n",
        "plot_F_Scores(y_test, y_predict_nb_unigram)\n",
        "plot_Confusion_Matrix(y_test, y_predict_nb_unigram, \"Oranges\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gHI5Qgefvt-b"
      },
      "cell_type": "markdown",
      "source": [
        "A score of 92% for F1 (weighted) is not too bad. Anyhow it is not sufficient and we will do better.\n",
        "\n",
        "And as suggested, it is pretty hard to distinguish between languages of the same familiy.\n",
        "\n",
        "Here our major problem is to distinguish between Dutch, English and Swedish.\n",
        "\n",
        "## Top 1% Mixture\n",
        "\n",
        "In the case of top 1% 1- & 2-Grams we are working on 3,079 features."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "hjZH27_pvt-b"
      },
      "cell_type": "code",
      "source": [
        "# Top 1%\n",
        "X_top1Percent_train, X_top1Percent_test = normalizeData(X_top1Percent_train_raw, X_top1Percent_test_raw)\n",
        "y_predict_nb_top1Percent = applyNaiveBayes(X_top1Percent_train, y_train, X_top1Percent_test)\n",
        "plot_F_Scores(y_test, y_predict_nb_top1Percent)\n",
        "plot_Confusion_Matrix(y_test, y_predict_nb_top1Percent, \"Reds\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0EGl-98Fvt-b"
      },
      "cell_type": "markdown",
      "source": [
        "F1 (weighted) of 97.46% is much better. Again we have some issues with European languages here.\n",
        "\n",
        "Unfortunately, we are still working on a lot of features.\n",
        "\n",
        "Let's see, if the Mixture of the Top 50 will perform better.\n",
        "\n",
        "## Top 50 Mixture\n",
        "We are using **564** features only."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "_YoFSzIvvt-c"
      },
      "cell_type": "code",
      "source": [
        "# Top 50\n",
        "X_top50_train, X_top50_test = normalizeData(X_top50_train_raw, X_top50_test_raw)\n",
        "y_predict_nb_top50 = applyNaiveBayes(X_top50_train, y_train, X_top50_test)\n",
        "plot_F_Scores(y_test, y_predict_nb_top50)\n",
        "plot_Confusion_Matrix(y_test, y_predict_nb_top50, \"Greens\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kVihWB9lvt-c"
      },
      "cell_type": "markdown",
      "source": [
        "Well, the result for F1 (weighted) of 97.52% is somehow better, but not significantly.\n",
        "\n",
        "Anyway we are using much fewer features. Therefore I will prefer this approach.\n",
        "\n",
        "## Comparison\n",
        "For Naive Bayes we achieve the following scores for F1 (weighted):\n",
        "- Unigram: 0.9201996483569281 (using 6,816 features)\n",
        "- Top 1% Mixture: 0.9746025107937131 (3,079 features)\n",
        "- Top 50 Mixture: 0.9751510056992273 (564 features)\n",
        "\n",
        "Now let's see, how k-Nearest-Neighbours performs.\n",
        "\n",
        "# k Nearest Neighbour\n",
        "First, we need to define the apply method and to discuss the size of k.\n",
        "\n",
        "The default for k=5. Let's stick to that default.\n",
        "\n",
        "I will run kNN on the Top-50-feature-set only. (If you like to run it on the Uni-Grams please uncomment below.)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "YUaIILRavt-c"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def applyNearestNeighbour(X_train, y_train, X_test):\n",
        "    trainArray = toNumpyArray(X_train)\n",
        "    testArray = toNumpyArray(X_test)\n",
        "\n",
        "    clf = KNeighborsClassifier()\n",
        "    clf.fit(trainArray, y_train)\n",
        "    y_predict = clf.predict(testArray)\n",
        "    return y_predict\n",
        "\n",
        "\n",
        "## Unigrams\n",
        "#y_predict_knn_unigram = applyNearestNeighbour(X_unigram_train, y_train, X_unigram_test)\n",
        "#plot_F_Scores(y_test, y_predict_knn_unigram)\n",
        "#plot_Confusion_Matrix(y_test, y_predict_knn_unigram, \"Purples\")\n",
        "\n",
        "# Top 50\n",
        "y_predict_knn_top50 = applyNearestNeighbour(X_top50_train, y_train, X_top50_test)\n",
        "plot_F_Scores(y_test, y_predict_knn_top50)\n",
        "plot_Confusion_Matrix(y_test, y_predict_knn_top50, \"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lkuVwocovt-c"
      },
      "cell_type": "markdown",
      "source": [
        "The results of kNN are almost equal to those of the Naive Bayes approach.\n",
        "\n",
        "In a future notebook, I might investigate some Auto-ML or hyperparameter optimization using the classical scikit algorithms.\n",
        "\n",
        "But, right now I am more interested in some analytical algorithms.\n",
        "\n",
        "\n",
        "# Analytical Algorithms\n",
        "\n",
        "*Is it possible to use an classical analytical algorithm to predict the language?*\n",
        "\n",
        "Well, this should work, since we are working on something very similar to a distribution.\n",
        "\n",
        "If we would restrict ourselves to a fixed n (for the n-Grams) only, it would be a real distribution.\n",
        "\n",
        "But for the Mixture case, this does not hold. E.g. the occurence of 'e' and 'n' and 'en' will not be independent.\n",
        "\n",
        "Anyhow, I will convert all inputs into a relative form, so that we do not run into technical problems.\n",
        "\n",
        "Lets inspect, if the following algorithms work here:\n",
        "- Least-Squares\n",
        "- Kulback-Leibler Divergence\n",
        "- Kolmogorov-Smirnov-Test\n",
        "\n",
        "\n",
        "\n",
        "## Assumptions\n",
        "- We are working on discrete distributions\n",
        "- $p_l(i)$ is the probability function of language $l$ for character/symbol $i$\n",
        "- $q(i)$ is the probability function of the current sample\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "exXwd4iSvt-d"
      },
      "cell_type": "code",
      "source": [
        "# Train lang dict\n",
        "def toRelative(X_test):\n",
        "    return [v / np.sum(v) for v in X_test]\n",
        "\n",
        "language_dict_top50 = train_lang_dict(X_top50_train, y_train.values)\n",
        "X_top50_test_rel = toRelative(X_top50_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AkGokjGvt-d"
      },
      "cell_type": "markdown",
      "source": [
        "`language_dict_top50` contains the relative distribution per language for the Top-50-feature-set. ($p_l(i)$ for all languages $l$)\n",
        "\n",
        "`X_top50_test_rel` contains (distribution-like) relative values for the test data, based on the Top-50-feature-set. ($q(i)$ for all samples)\n",
        "\n",
        "# (Ordinary) least squares\n",
        "Identify the language by taking the minimum of the sum of the squared differnces per feature (=character/symbol), i.e.\n",
        "\n",
        "$$ \\delta_l = \\sum _i (p_l(i)-q(i))^2 $$\n",
        "$$ l^* = argmin_l(\\delta_l) $$"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "mDDtzc7-vt-d"
      },
      "cell_type": "code",
      "source": [
        "def ols_predict(language_dict, X_test):\n",
        "    def calcSquareDifference(p, q):\n",
        "        return np.sum((p-q)**2)\n",
        "\n",
        "    def ols(language_dict, v, langs):\n",
        "        olsVector = np.array([calcSquareDifference([language_dict[l]], v) for l in langs])\n",
        "        index = np.argmin(olsVector)\n",
        "        return langs[index]\n",
        "\n",
        "    langs = [l for l in language_dict]\n",
        "    return [ols(language_dict, v, langs) for v in X_test]\n",
        "\n",
        "\n",
        "ols_predictions = ols_predict(language_dict_top50, X_top50_test_rel)\n",
        "\n",
        "plot_F_Scores(y_test, ols_predictions)\n",
        "plot_Confusion_Matrix(y_test, ols_predictions, 'cool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CgAZvbOfvt-d"
      },
      "cell_type": "markdown",
      "source": [
        "It seems, that OLS results in more differences to the labelled data than Naive Bayes and k-Nearest-Neighbour.\n",
        "\n",
        "Differences are:\n",
        "- Latin -> predicted as English (seems to be a common error also for Naive Bayes and kNN)\n",
        "- Urdu -> predicted as Arabic\n",
        "- Chinese -> English\n",
        "- Spanish -> Portuguese\n",
        "\n",
        "\n",
        "# Kullback Leibler Divergence\n",
        "Taking the smallest value of [Kullback Leibler Divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) as a measure for the difference between distributions, given by:\n",
        "$$D_\\text{KL}(p \\parallel q) = \\sum_{x\\in\\mathcal{X}} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right)$$"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "r31cIkjWvt-e"
      },
      "cell_type": "code",
      "source": [
        "# Kullback Leibler Divergence\n",
        "from math import log\n",
        "\n",
        "\n",
        "def kl_predict(language_dict, X_test):\n",
        "\n",
        "    def kl_divergence(p, q):\n",
        "        p_ = np.array(p) + 1e-200\n",
        "        q_ = np.array(q) + 1e-200\n",
        "        n = len(p)\n",
        "        return np.sum([p_[i] * log(p_[i]/ (q_[i] )) for i in range(n)])\n",
        "\n",
        "    def predict(language_dict, v, langs):\n",
        "        divs = [kl_divergence(language_dict[l], v) for l in langs]\n",
        "        index = np.argmin(divs)\n",
        "        return langs[index]\n",
        "\n",
        "    langs = [l for l in language_dict]\n",
        "    return [predict(language_dict, v, langs) for v in X_test]\n",
        "\n",
        "\n",
        "kl_predictions = kl_predict(language_dict_top50, X_top50_test_rel)\n",
        "\n",
        "plot_F_Scores(y_test, kl_predictions)\n",
        "plot_Confusion_Matrix(y_test, kl_predictions, 'plasma')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cx4gOh4Xvt-e"
      },
      "cell_type": "markdown",
      "source": [
        "In addition to the observed errors for OLS, the KL approach has some more errors on:\n",
        "- Pushto vs. Persian\n",
        "- Persian vs. Urdu\n",
        "- Latin vs. French\n",
        "\n",
        "# Kolmogorov Smirnov Test\n",
        "\n",
        "Use the two-sample-case of the [Kolmogorov Smirnov Test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test#Discrete_and_mixed_null_distribution) to check, if a given sample is written in a specific language.\n",
        "\n",
        "The general formula is given by:\n",
        "\n",
        "$$ D_{n,m}=\\sup_x |F_{1,n}(x)-F_{2,m}(x)| $$\n",
        "\n",
        "We then reject the Null hypothesis ($H_0$) at level $\\alpha$ if\n",
        "$$ D_{n,m}>c(\\alpha)\\sqrt{\\frac{n + m}{n\\cdot m}}. $$\n",
        "\n",
        "Where\n",
        "- $F_1$ and $F_2$ are two samples\n",
        "- consisting of $m$ respective $n$ samples\n",
        "\n",
        "* Applied in our case:\n",
        "\n",
        "$$ D^l_{n,m}=\\sup_i |P(i)-Q(i)| $$\n",
        "\n",
        "for each language $l$.\n",
        "\n",
        "\n",
        "We have in total 1,000 texts per language (20% = 800 for training). So, $ n \\approx 800 * AvgChars $\n",
        "\n",
        "$\\alpha=0.01 \\implies c(\\alpha)=1.628 $\n",
        "\n",
        "Since we might reject the Null hypothesis for all languages, we could also cover the case, that the sample is written in a completely different language.\n",
        "\n",
        "In the case when $H_0$ holds for more languages, we will use the language $l$ with the smallest value for $D^l$.\n",
        "\n",
        "In other words: *We will choose the language, where the maximum of all absolute differences is the smallest. Except the Null hypothesis does not hold.*"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "9ZDG4z58vt-k"
      },
      "cell_type": "code",
      "source": [
        "# Calculate average number of chars per Text\n",
        "avgChars = np.mean(raw.apply(lambda x : len(x['Text']), axis=1))\n",
        "print('avgChars', avgChars)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "-tTbPLv4vt-k"
      },
      "cell_type": "code",
      "source": [
        "def ks_predict(language_dict, X_test, n, c_alpha=1.628):\n",
        "    def calcMaxAbsDifference(p, q):\n",
        "        return np.max(np.abs((p-q)))\n",
        "\n",
        "    def scaleAlpha(n, m, c_alpha):\n",
        "        factor = ((n + m) / n / m) ** 0.5\n",
        "        return factor * c_alpha\n",
        "\n",
        "    def ks(language_dict, v, langs):\n",
        "        ksVector = np.array([calcMaxAbsDifference([language_dict[l]], v) for l in langs])\n",
        "        index = np.argmin(ksVector)\n",
        "        m = len(v)\n",
        "        scaledAlpha = scaleAlpha(n, m, c_alpha)\n",
        "        if ksVector[index] <= scaledAlpha:\n",
        "            return langs[index]\n",
        "        else:\n",
        "            return '_N/A_'\n",
        "\n",
        "    langs = [l for l in language_dict]\n",
        "    return [ks(language_dict, v, langs) for v in X_test]\n",
        "\n",
        "\n",
        "ks_predictions = ks_predict(language_dict_top50, X_top50_test_rel, 356*800)\n",
        "print('None-Predicitons:', (np.array(ks_predictions)=='_N/A_').sum())\n",
        "plot_F_Scores(y_test, ks_predictions)\n",
        "plot_Confusion_Matrix(y_test, ks_predictions, 'summer')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mDNyV0c5vt-k"
      },
      "cell_type": "markdown",
      "source": [
        "In addition to the problems observed for OLS and KL we are facing a lot of more problems here.\n",
        "\n",
        "\n",
        "# Comparison of results\n",
        "\n",
        "Before we finally compare all results, we start with the implementation of a benchmark using an existing library. And have a quick look on the mismatched data.\n",
        "\n",
        "\n",
        "## Benchmark library `langdetect`\n",
        "\n",
        "Unfortunately, we need to add some fix-up code:\n",
        "- the [output](https://pypi.org/project/langdetect/) of `langdetect` is in iso-639 format (except for Chinese).\n",
        "- Italian is called Latin in our dataset\n",
        "- Portuguese is named Portugese in our dataset"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "cn_q-f_5vt-l"
      },
      "cell_type": "code",
      "source": [
        "! pip install langdetect\n",
        "! pip install iso-639\n",
        "\n",
        "from langdetect import detect\n",
        "from iso639 import languages\n",
        "\n",
        "def proofLanguage(text):\n",
        "    twoLetterCode = detect(text)[:2] #consolidate Chinese\n",
        "    if twoLetterCode == 'it': # Italian -> Latin\n",
        "        return 'Latin'\n",
        "    elif twoLetterCode == 'pt': # Portuguese -> Portugese\n",
        "        return 'Portugese'\n",
        "    else:\n",
        "        lang = languages.get(alpha2=twoLetterCode)\n",
        "        langName = lang.name\n",
        "        if not langName==None:\n",
        "            return langName\n",
        "        return twoLetterCode\n",
        "\n",
        "y_test_proof = [proofLanguage(t) for t in X_test]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "wwO3CrfVvt-l"
      },
      "cell_type": "code",
      "source": [
        "plot_F_Scores(y_test, y_test_proof)\n",
        "plot_Confusion_Matrix(y_test, y_test_proof, 'cool')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilN3BSrIvt-l"
      },
      "cell_type": "markdown",
      "source": [
        "Obviously the benchmark library has some large differences in the prediction:\n",
        "- Pushto and Persian are both predicted to be Persian.\n",
        "- Some Italian (here Latin) is classified as Catalan, English, French, Romanian, ...\n",
        "- Chinese is also predicted as Korean.\n",
        "- Texts labelled to be in Urdu are classified to be Arabic.\n",
        "\n",
        "\n",
        "***\n",
        "\n",
        "Well, the benchmark seems to be beaten by far. But this does not reflect the reality.\n",
        "\n",
        "Let us have a look on the occured differences.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "R0Ee07_1vt-l"
      },
      "cell_type": "markdown",
      "source": [
        "## Error Analysis\n",
        "Lets have a brief look on the error details for some selected scenarios.\n",
        "\n",
        "First we define a helper method `plotTopErrors`:"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ZYNtkbtMvt-l"
      },
      "cell_type": "code",
      "source": [
        "def plotTopErrors(y_predict, top=5):\n",
        "    ys = y_test.values\n",
        "    Xs = X_test.values\n",
        "    errorCount = 0\n",
        "\n",
        "    for i in range(len(ys)):\n",
        "        if not ys[i]==y_predict[i]:\n",
        "            errorCount += 1\n",
        "            print(\"#{}: Expected: {}, Predicted: {}\".format(errorCount, ys[i], y_predict[i]))\n",
        "            print(\"Text:\", Xs[i])\n",
        "            print(\"=================================================\")\n",
        "        if errorCount >= top:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "49VplLDSvt-l"
      },
      "cell_type": "markdown",
      "source": [
        "Now, lets start with the error inspection.\n",
        "\n",
        "### Naive Bayes on Mix Top 50"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AOqbhXXBvt-l"
      },
      "cell_type": "code",
      "source": [
        "plotTopErrors(y_predict_nb_top50, top=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nq6b0A6Bvt-m"
      },
      "cell_type": "markdown",
      "source": [
        "Well, there are errors on both sides. But as far as I can see, the majority of the predicted values is correct.\n",
        "\n",
        "So far I would suggest, that the real F1 is better than 97%.\n",
        "\n",
        "Anyhow, I need to proof my suggestion in the future.\n",
        "\n",
        "### Naive Bayes on Uni-Grams"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "FxS7UiH4vt-m"
      },
      "cell_type": "code",
      "source": [
        "plotTopErrors(y_predict_nb_unigram, top=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "25c3xXNRvt-m"
      },
      "cell_type": "markdown",
      "source": [
        "Using Uni-Grams, does result in the initially expected errors.\n",
        "\n",
        "Well, it is very hard to distinguish between similar languages.\n",
        "\n",
        "### Kolmogorov Smirnov on Mix Top 50"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "pW8_pBBEvt-m"
      },
      "cell_type": "code",
      "source": [
        "plotTopErrors(ks_predictions, top=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IiTKywplvt-m"
      },
      "cell_type": "markdown",
      "source": [
        "Well, it seems that a lot of the predicted values are wrong for KS."
      ]
    },
    {
      "metadata": {
        "id": "ES7v2XCTvt-m"
      },
      "cell_type": "markdown",
      "source": [
        "## Comparison Ranking by F1 weighted\n",
        "1. 0.9751510056992273 **NB** on **Mix Top 50**\n",
        "1. 0.9746025107937131 **NB** on **Mix Top 1%**\n",
        "1. 0.9723506120636130 **kNN** on **Mix Top 50**\n",
        "1. 0.9645429434449447 **KL** on **Mix Top 50**\n",
        "1. 0.9495364489332023 **OLS** on **Mix Top 50**\n",
        "1. 0.9201996483569281 **NB** on **Unigrams**\n",
        "1. 0.8789520584453827 **langdetect**\n",
        "1. 0.8025836015759978 **KS** on **Mix Top 50**"
      ]
    },
    {
      "metadata": {
        "id": "EEnJUFOBvt-n"
      },
      "cell_type": "markdown",
      "source": [
        "# Future Work & Next Steps\n",
        "- Rerun analysis on cleaned data\n",
        "- Use quantitative method to identify features, e.g. PCA\n",
        "- Improve results by Hyperparameter-Optimzation (at least run some Auto-ML)\n",
        "- Try to identify, if there are two languages. And try to predict the secondary language"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Language Detection",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}